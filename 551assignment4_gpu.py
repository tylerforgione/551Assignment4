# -*- coding: utf-8 -*-
"""551Assignment4_gpu

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15_0cHzlRwR_5TGmtXXa6bN7gJ7bCLnFc
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, GPT2Tokenizer, GPT2Model, GPT2LMHeadModel, AutoTokenizer, AutoModel, AdamW, AutoModelForCausalLM, Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoConfig
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from scipy.stats import zscore
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer
from sklearn.feature_selection import VarianceThreshold
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from tqdm import tqdm
import torch.optim as optim
from torch.optim import AdamW
from torch.nn import CrossEntropyLoss
import os
os.environ["WANDB_DISABLED"] = "true"
import seaborn as sns

!pip install datasets
from datasets import Dataset

socs_model = 'distilgpt2'
bert_model = 'bert-base-uncased'

"""# Task 1: Preprocess Data

## Collecting data using pandas
"""

splits = {'train': 'simplified/train-00000-of-00001.parquet', 'validation': 'simplified/validation-00000-of-00001.parquet', 'test': 'simplified/test-00000-of-00001.parquet'}
train_set = pd.read_parquet("hf://datasets/google-research-datasets/go_emotions/" + splits["train"])
val_set = pd.read_parquet("hf://datasets/google-research-datasets/go_emotions/" + splits["validation"])
test_set = pd.read_parquet("hf://datasets/google-research-datasets/go_emotions/" + splits["test"])

"""Dropping all instances with more than 1 label"""

train_set = train_set[train_set.labels.apply(len) == 1]
val_set = val_set[val_set.labels.apply(len) == 1]
test_set = test_set[test_set.labels.apply(len) == 1]

"""Counting class distribution and plotting it"""

# Concatenate all datasets
combined_set = pd.concat([train_set, val_set, test_set])

# Count the occurrences of each class
class_counts = combined_set['labels'].value_counts()

# Calculate total number of samples
total_samples = len(combined_set)

# Calculate percentage distribution for each class
class_distribution = class_counts / total_samples * 100

# Display results
print("Class Counts:")
print(class_counts)
print("\nClass Distribution (%):")
print(class_distribution)

"""wow! mostly neutral! (makes sense irl)"""

class_counts = {
    27: 16021,
    0: 3384,
    15: 2378,
    4: 2367,
    1: 2046,
    10: 1809,
    3: 1809,
    18: 1760,
    7: 1729,
    2: 1265,
    20: 1087,
    6: 1057,
    17: 1052,
    25: 1003,
    26: 902,
    9: 888,
    5: 831,
    22: 749,
    11: 635,
    13: 619,
    14: 553,
    8: 497,
    24: 437,
    12: 246,
    19: 105,
    23: 103,
    21: 67,
    16: 47
}

class_counts_series = pd.Series(class_counts)

# Calculate percentage distribution
total_samples = class_counts_series.sum()
class_distribution = class_counts_series / total_samples * 100

# Plot the class distribution
plt.figure(figsize=(8, 4))
plt.style.use('ggplot')
plt.bar(class_distribution.index, class_distribution.values)

# Formatting the plot
plt.xlabel("Class")
plt.ylabel("Percentage (%)")
plt.title("Class Distribution in Combined Dataset")
plt.xticks(class_distribution.index)
plt.tight_layout()
plt.show()

"""### Selecting highly relevant features for baseline model"""

vectorizer = TfidfVectorizer(stop_words='english', max_features=30) # remove stopwords
train_inp = vectorizer.fit_transform(train_set['text'])
val_inp = vectorizer.transform(val_set['text'])
test_inp = vectorizer.transform(test_set['text'])

train_zscore = zscore(train_inp.toarray(), axis=0) # get zscore of each instance
val_zscore = zscore(val_inp.toarray(), axis=0)
test_zscore = zscore(test_inp.toarray(), axis=0)

selector = VarianceThreshold(threshold=0.01)  # Minimum variance threshold
train_selected = selector.fit_transform(train_zscore)
val_selected = selector.transform(val_zscore)
test_selected = selector.transform(test_zscore)

"""### Turning unstructured data into numerical features for Naive Bayes"""

vectorizer = CountVectorizer(stop_words='english')
train_inp_bayes = vectorizer.fit_transform(train_set['text'])
val_inp_bayes = vectorizer.fit_transform(val_set['text'])
test_inp_bayes = vectorizer.fit_transform(test_set['text'])

mlb = MultiLabelBinarizer()

y_train = mlb.fit_transform(train_set['labels'])
y_val = mlb.fit_transform(val_set['labels'])
y_test = mlb.fit_transform(test_set['labels'])

"""### Tokenizing input text for LLM"""

tokenizer = AutoTokenizer.from_pretrained(socs_model)
tokenizer.pad_token = tokenizer.eos_token

train_set["tokenized"] = train_set["text"].apply(
    lambda x: tokenizer(x, padding="max_length", truncation=True, return_tensors="pt", max_length=128)
)

# Extract tokenized columns into new DataFrame columns
train_set["input_ids"] = train_set["tokenized"].apply(lambda x: x["input_ids"].squeeze(0))
train_set["attention_mask"] = train_set["tokenized"].apply(lambda x: x["attention_mask"].squeeze(0))

# Convert the padded sequences to PyTorch tensors
input_ids = torch.stack(list(train_set["input_ids"]))
attention_mask = torch.stack(list(train_set["attention_mask"]))
labels = torch.tensor(train_set["labels"].tolist())

test_set["tokenized"] = test_set["text"].apply(
    lambda x: tokenizer(x, padding="max_length", truncation=True, return_tensors="pt", max_length=128)
)

# Extract tokenized columns into new DataFrame columns
test_set["input_ids"] = test_set["tokenized"].apply(lambda x: x["input_ids"].squeeze(0))
test_set["attention_mask"] = test_set["tokenized"].apply(lambda x: x["attention_mask"].squeeze(0))

# Convert the padded sequences to PyTorch tensors
input_ids2 = torch.stack(list(test_set["input_ids"]))
attention_mask2 = torch.stack(list(test_set["attention_mask"]))
labels2 = torch.tensor(test_set["labels"].tolist())

val_set["tokenized"] = val_set["text"].apply(
    lambda x: tokenizer(x, padding="max_length", truncation=True, return_tensors="pt", max_length=128)
)

# Extract tokenized columns into new DataFrame columns
val_set["input_ids"] = val_set["tokenized"].apply(lambda x: x["input_ids"].squeeze(0))
val_set["attention_mask"] = val_set["tokenized"].apply(lambda x: x["attention_mask"].squeeze(0))

# Convert the padded sequences to PyTorch tensors
input_ids3 = torch.stack(list(val_set["input_ids"]))
attention_mask3 = torch.stack(list(val_set["attention_mask"]))
labels3 = torch.tensor(val_set["labels"].tolist())

train_dataset = Dataset.from_dict({
    "input_ids": input_ids.tolist(),
    "attention_mask": attention_mask.tolist(),
    "labels": labels.tolist(),
})

test_dataset = Dataset.from_dict({
    "input_ids": input_ids2.tolist(),
    "attention_mask": attention_mask2.tolist(),
    "labels": labels2.tolist(),
})

val_dataset = Dataset.from_dict({
    "input_ids": input_ids3.tolist(),
    "attention_mask": attention_mask3.tolist(),
    "labels": labels3.tolist(),
})

"""# Task 3: Run Experiments

### I am now testing the pretrained model with NO finetuning. Essentially, this result is based on what the pretrained model outputs.
"""

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    accuracy = (predictions == labels).mean()
    return {"accuracy": accuracy}

eval_args = TrainingArguments(
    output_dir="./results",
    per_device_eval_batch_size=16,
    do_train=False,  # skip training
    do_eval=True,
    logging_dir='./logs',
    logging_steps=10
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = AutoModelForSequenceClassification.from_pretrained(socs_model, num_labels=28).to(device)
tokenizer = AutoTokenizer.from_pretrained(socs_model)
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.pad_token_id

trainer = Trainer(
    model=model,
    args=eval_args,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics if labels2 is not None else None
)

predictions = trainer.predict(test_dataset)
logits = predictions.predictions
predicted_classes = np.argmax(logits, axis=-1)

if labels2 is not None:
    accuracy = (predicted_classes == labels2.numpy()).mean()
    print(f"Accuracy: {accuracy * 100:.2f}%")

"""### Here, I am finetuning the model's classification layer (the final layer) over 10 epochs."""

def finetune_last(lr=5e-3, batchsize=16, val=True):
  if val:
    eval_data = val_dataset
  else:
    eval_data = test_dataset
  # Load the model
  model = AutoModelForSequenceClassification.from_pretrained(socs_model, num_labels=28)
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  model.to(device)
  model.config.pad_token_id = tokenizer.pad_token_id

  # Freeze all parameters except the classification head
  for param in model.base_model.parameters():
      param.requires_grad = False

  def compute_metrics(pred):
      logits, labels = pred
      predictions = torch.argmax(torch.tensor(logits), dim=-1).numpy()
      precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average="weighted")
      acc = accuracy_score(labels, predictions)
      return {"eval_accuracy": acc, "recall": recall, "precision": precision, "f1": f1}

  training_args = TrainingArguments(
      output_dir="./results",        # Save directory
      evaluation_strategy="epoch",  # Evaluate at the end of each epoch
      save_strategy="epoch",        # Save model at each epoch
      logging_dir="./logs",         # Directory for logs
      logging_strategy="steps",     # Log every few steps
      logging_steps=500,
      per_device_train_batch_size=batchsize, # Adjust batch size based on your GPU memory
      per_device_eval_batch_size=batchsize,
      num_train_epochs=10,
      learning_rate=lr,
      save_total_limit=2,           # Save only the 2 most recent checkpoints
      load_best_model_at_end=True,  # Automatically load the best model at the end
      metric_for_best_model="accuracy",
  )

  # Initialize Trainer
  trainer = Trainer(
      model=model,
      args=training_args,
      train_dataset=train_dataset,
      eval_dataset=eval_data,
      tokenizer=tokenizer,  # Include tokenizer if you use it for padding/truncation
      compute_metrics=compute_metrics,
  )

  # Train the model
  trainer.train()

finetune_last(1e-5)
finetune_last(1e-4)
finetune_last(5e-4)
finetune_last(1e-3)
finetune_last(5e-3)

# Define the data for each learning rate (each is a list of tuples: (epoch, training_loss, validation_loss, accuracy))
data_lr_1e5 = [
    (1, 2.847600, 2.650651, 0.342266),
    (2, 2.576100, 2.544970, 0.348584),
    (3, 2.541100, 2.487882, 0.356209),
    (4, 2.468800, 2.449571, 0.366449),
    (5, 2.448400, 2.419134, 0.374728),
    (6, 2.441400, 2.396918, 0.378867),
    (7, 2.387400, 2.381229, 0.380610),
    (8, 2.393700, 2.370609, 0.382135),
    (9, 2.391600, 2.364534, 0.383878),
    (10, 2.379200, 2.362538, 0.384532)
]

data_lr_1e4 = [
    (1, 2.326100, 2.225478, 0.404575),
    (2, 2.145300, 2.109482, 0.420044),
    (3, 2.095000, 2.044553, 0.430065),
    (4, 2.017400, 2.009133, 0.437691),
    (5, 1.997000, 1.984640, 0.444009),
    (6, 1.982900, 1.968754, 0.445752),
    (7, 1.937000, 1.951797, 0.449891),
    (8, 1.937700, 1.943659, 0.450763),
    (9, 1.928700, 1.938945, 0.450980),
    (10, 1.920100, 1.937457, 0.452505)
]

data_lr_5e4 = [
    (1, 2.075200, 1.997292, 0.445534),
    (2, 1.930900, 1.936251, 0.457081),
    (3, 1.885900, 1.868981, 0.467538),
    (4, 1.817200, 1.852068, 0.473203),
    (5, 1.795700, 1.857842, 0.458388),
    (6, 1.785700, 1.829479, 0.472549),
    (7, 1.742700, 1.811168, 0.475163),
    (8, 1.734200, 1.800907, 0.477778),
    (9, 1.725300, 1.797947, 0.481264),
    (10, 1.698100, 1.795707, 0.479085)
]

data_lr_1e3 = [
    (1, 2.036600, 1.949115, 0.454684),
    (2, 1.908200, 1.935656, 0.443137),
    (3, 1.859700, 1.864128, 0.467102),
    (4, 1.793600, 1.846294, 0.475163),
    (5, 1.764400, 1.860423, 0.462309),
    (6, 1.753800, 1.822802, 0.475817),
    (7, 1.704000, 1.812150, 0.474292),
    (8, 1.686900, 1.789309, 0.481046),
    (9, 1.673400, 1.788957, 0.485839),
    (10, 1.635100, 1.779365, 0.484967)
]

data_lr_5e3 = [
    (1, 2.485800, 2.374017, 0.417429),
    (2, 2.349200, 2.226635, 0.444880),
    (3, 2.225300, 2.196420, 0.441830),
    (4, 2.089900, 2.318052, 0.388017),
    (5, 2.022000, 2.111471, 0.464488),
    (6, 1.950400, 2.095941, 0.450763),
    (7, 1.845400, 1.968017, 0.455556),
    (8, 1.766800, 1.897171, 0.476471),
    (9, 1.702800, 1.901714, 0.468192),
    (10, 1.588400, 1.835360, 0.494336)
]

# Convert the data into numpy arrays for easier plotting
epochs = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

def extract_column(data, column_index):
    return np.array([x[column_index] for x in data])

# Extracting the columns for training loss, validation loss, and accuracy for each learning rate
train_loss_1e5 = extract_column(data_lr_1e5, 1)
val_loss_1e5 = extract_column(data_lr_1e5, 2)
accuracy_1e5 = extract_column(data_lr_1e5, 3)

train_loss_1e4 = extract_column(data_lr_1e4, 1)
val_loss_1e4 = extract_column(data_lr_1e4, 2)
accuracy_1e4 = extract_column(data_lr_1e4, 3)

train_loss_5e4 = extract_column(data_lr_5e4, 1)
val_loss_5e4 = extract_column(data_lr_5e4, 2)
accuracy_5e4 = extract_column(data_lr_5e4, 3)

train_loss_1e3 = extract_column(data_lr_1e3, 1)
val_loss_1e3 = extract_column(data_lr_1e3, 2)
accuracy_1e3 = extract_column(data_lr_1e3, 3)

train_loss_5e3 = extract_column(data_lr_5e3, 1)
val_loss_5e3 = extract_column(data_lr_5e3, 2)
accuracy_5e3 = extract_column(data_lr_5e3, 3)

# Plotting
fig, axs = plt.subplots(1, 2, figsize=(12, 4))

# Plot Training and Validation Losses
axs[0].plot(epochs, train_loss_1e5, label='Train Loss (1e-5)', color='blue')
axs[0].plot(epochs, val_loss_1e5, label='Val Loss (1e-5)', color='cyan', linestyle='--')
axs[0].plot(epochs, train_loss_1e4, label='Train Loss (1e-4)', color='green')
axs[0].plot(epochs, val_loss_1e4, label='Val Loss (1e-4)', color='lime', linestyle='--')
axs[0].plot(epochs, train_loss_5e4, label='Train Loss (5e-4)', color='red')
axs[0].plot(epochs, val_loss_5e4, label='Val Loss (5e-4)', color='orange', linestyle='--')
axs[0].plot(epochs, train_loss_1e3, label='Train Loss (1e-3)', color='purple')
axs[0].plot(epochs, val_loss_1e3, label='Val Loss (1e-3)', color='violet', linestyle='--')
axs[0].plot(epochs, train_loss_5e3, label='Train Loss (5e-3)', color='black')
axs[0].plot(epochs, val_loss_5e3, label='Val Loss (5e-3)', color='grey', linestyle='--')

axs[0].set_title('Training and Validation Loss')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Loss')
axs[0].legend(fontsize='8', framealpha=0.3)

# Plot Accuracy
axs[1].plot(epochs, accuracy_1e5, label='Val Accuracy (1e-5)', color='blue')
axs[1].plot(epochs, accuracy_1e4, label='Val Accuracy (1e-4)', color='green')
axs[1].plot(epochs, accuracy_5e4, label='Val Accuracy (5e-4)', color='red')
axs[1].plot(epochs, accuracy_1e3, label='Val Accuracy (1e-3)', color='purple')
axs[1].plot(epochs, accuracy_5e3, label='Val Accuracy (5e-3)', color='black')

axs[1].set_title('Accuracy per Learning Rate')
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracy')
axs[1].legend()

# Adjust layout for better spacing
plt.tight_layout()
plt.show()

finetune_last(5e-3, 8)
finetune_last(5e-3, 16)
finetune_last(5e-3, 32)
finetune_last(5e-3, 64)
finetune_last(5e-3, 128)

batch_sizes = [8, 16, 32, 64, 128]

# Epochs
epochs = np.arange(1, 11)

# Training Loss
training_loss = [
    [2.906200, 2.724800, 2.583900, 2.434000, 2.244500, 2.107300, 2.001100, 1.820200, 1.747100, 1.621400],
    [2.485800, 2.349200, 2.225300, 2.089900, 2.022000, 1.950400, 1.845400, 1.766800, 1.702800, 1.588400],
    [2.277000, 2.116100, 2.049100, 1.957900, 1.906000, 1.839500, 1.779200, 1.717300, 1.671600, 1.619900],
    [2.247700, 2.014000, 1.923800, 1.905200, 1.828500, 1.808800, 1.765500, 1.698000, 1.663000, 1.621400],
    [2.081500, 2.081500, 2.081500, 1.855700, 1.855700, 1.776000, 1.776000, 1.726900, 1.676600, 1.676600]
]

# Validation Loss
validation_loss = [
    [2.794266, 2.866201, 2.442700, 2.404255, 2.463467, 2.234912, 2.045345, 2.098076, 1.956355, 1.838048],
    [2.374017, 2.226635, 2.196420, 2.318052, 2.111471, 2.095941, 1.968017, 1.897171, 1.901714, 1.835360],
    [2.062704, 2.173868, 2.063353, 2.005800, 1.927015, 1.973334, 1.903558, 1.816659, 1.840054, 1.768400],
    [2.029486, 2.040059, 1.956786, 1.897839, 1.917616, 1.878406, 1.851265, 1.798107, 1.784056, 1.757154],
    [1.967557, 1.994102, 1.847763, 1.893551, 1.887886, 1.883098, 1.777413, 1.778229, 1.765082, 1.745373]
]

# Accuracy
accuracy = [
    [0.434037, 0.390941, 0.425242, 0.421064, 0.417986, 0.433597, 0.479112, 0.432058, 0.470097, 0.498021],
    [0.417429, 0.444880, 0.441830, 0.388017, 0.464488, 0.450763, 0.455556, 0.476471, 0.468192, 0.494336],
    [0.424362, 0.395998, 0.436236, 0.457564, 0.474934, 0.433377, 0.457784, 0.493404, 0.476033, 0.498461],
    [0.452726, 0.440853, 0.445690, 0.473395, 0.471856, 0.481750, 0.477133, 0.492744, 0.495822, 0.500440],
    [0.456684, 0.459982, 0.481970, 0.478012, 0.463500, 0.480431, 0.494283, 0.495383, 0.499120, 0.500660]
]

# Plot Training and Validation Loss
plt.figure(figsize=(8, 4))

for i, batch_size in enumerate(batch_sizes):
    # Ignore None values for plotting
    valid_training_loss = [x for x in training_loss[i] if x is not None]
    valid_validation_loss = [x for x in validation_loss[i] if x is not None]
    epochs_trimmed = epochs[:len(valid_training_loss)]

    plt.plot(epochs_trimmed, valid_training_loss, label=f"Train Loss (Batch={batch_size})", linestyle='--')
    plt.plot(epochs_trimmed, valid_validation_loss, label=f"Val Loss (Batch={batch_size})")

plt.title("Training and Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(fontsize=8)
plt.grid(True)
plt.show()

# Plot Accuracy
plt.figure(figsize=(8, 4))

for i, batch_size in enumerate(batch_sizes):
    plt.plot(epochs, accuracy[i], label=f"Accuracy (Batch={batch_size})")

plt.title("Accuracy Across Epochs")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

"""### Finally, I am finetuning the entire model (all layers)"""

def finetune_full(lr=1e-5, batchsize=16, val=True, dropout=0.1, decay=0.01, average="weighted", model1=socs_model, epochs=5, save=False):
  if val:
    eval_data = val_dataset
  else:
    eval_data = test_dataset

  config = AutoConfig.from_pretrained(model1, num_labels=28)

  # Modify dropout rates
  config.hidden_dropout_prob = dropout  # Adjust dropout for hidden layers
  config.attention_probs_dropout_prob = dropout

  # Load the model
  model = AutoModelForSequenceClassification.from_pretrained(model1, config=config)
  tokenizer = AutoTokenizer.from_pretrained(socs_model)
  tokenizer.pad_token = tokenizer.eos_token
  model.config.pad_token_id = tokenizer.pad_token_id

  def compute_metrics(pred):
      logits, labels = pred
      predictions = torch.argmax(torch.tensor(logits), dim=-1).numpy()
      precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=average)
      acc = accuracy_score(labels, predictions)
      return {"eval_accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

  training_args = TrainingArguments(
      output_dir="./results",        # Save directory
      evaluation_strategy="epoch",  # Evaluate at the end of each epoch
      save_strategy="epoch",        # Save model at each epoch
      logging_dir="./logs",         # Directory for logs
      logging_strategy="steps",     # Log every few steps
      logging_steps=500,
      per_device_train_batch_size=batchsize, # Adjust batch size based on your GPU memory
      per_device_eval_batch_size=batchsize,
      num_train_epochs=epochs,
      weight_decay=decay,
      learning_rate=lr,
      save_total_limit=2,           # Save only the 2 most recent checkpoints
      load_best_model_at_end=True,  # Automatically load the best model at the end
      metric_for_best_model="accuracy",
  )

  # Initialize Trainer
  trainer = Trainer(
      model=model,
      args=training_args,
      train_dataset=train_dataset,
      eval_dataset=eval_data,
      tokenizer=tokenizer,  # Include tokenizer if you use it for padding/truncation
      compute_metrics=compute_metrics,
  )

  # Train the model
  trainer.train()

  if save:
    tokenizer.save_pretrained('/content/fine_tuned_distilgpt2')
    model.save_pretrained('/content/fine_tuned_distilgpt2')

finetune_full(2e-6, 16)
finetune_full(2e-5, 16)
finetune_full(2e-4, 16)
finetune_full(2e-3, 16)

data = [
    {
        "epoch": [1, 2, 3, 4, 5],
        "training_loss": [1.8673, 1.5936, 1.4146, 1.1962, 0.9921],
        "validation_loss": [1.73775, 1.626678, 1.623485, 1.702568, 1.845169],
        "accuracy": [0.53562, 0.560906, 0.559587, 0.547713, 0.538478]
    },
    {
        "epoch": [1, 2, 3, 4, 5],
        "training_loss": [1.5538, 1.3247, 1.2512, 1.1585, 1.1081],
        "validation_loss": [1.403133, 1.299911, 1.297571, 1.303477, 1.317898],
        "accuracy": [0.599824, 0.620932, 0.619173, 0.616315, 0.618294]
    },
    {
        "epoch": [1, 2, 3, 4, 5],
        "training_loss": [1.4381, 1.164, 0.8835, 0.5517, 0.3257],
        "validation_loss": [1.324514, 1.291467, 1.41184, 1.76845, 2.057516],
        "accuracy": [0.603122, 0.612137, 0.601583, 0.582454, 0.578496]
    },
    {
        "epoch": [1, 2, 3, 4, 5],
        "training_loss": [2.5052, 2.1502, 2.0157, 1.9106, 1.894],
        "validation_loss": [2.379929, 2.028955, 1.898232, 1.827933, 1.804739],
        "accuracy": [0.396878, 0.484169, 0.498021, 0.515172, 0.519349]
    }
]

# Create a 2x2 plot
fig, axs = plt.subplots(1, 2, figsize=(11, 5))

# Colors for different models
colors = ['b', 'g', 'r', 'm']
labels = ['2e-6', '2e-5', '2e-4', '2e-3']

# Plot training/validation loss
for i, model_data in enumerate(data):
    axs[0].plot(model_data['epoch'], model_data['training_loss'], f"{colors[i]}-", label=f"Training Loss ({labels[i]})")
    axs[0].plot(model_data['epoch'], model_data['validation_loss'], f"{colors[i]}--", label=f"Validation Loss ({labels[i]})")

axs[0].set_title("Training vs Validation Loss")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend(fontsize=8)
axs[0].grid(True)

# Plot accuracy
for i, model_data in enumerate(data):
    axs[1].plot(model_data['epoch'], model_data['accuracy'], f"{colors[i]}-", label=f"Accuracy ({labels[i]})")

axs[1].set_title("Accuracy")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Accuracy")
axs[1].legend(fontsize=8)
axs[1].grid(True)

plt.tight_layout()
plt.show()

"""Now testing over batch size (we already have 16 with the optimal learning rate so we use that)"""

finetune_full(2e-5, 32)
finetune_full(2e-5, 64)

data = [
    {
        "epoch": [1, 2, 3, 4, 5],
        "training_loss": [1.5538, 1.3247, 1.2512, 1.1585, 1.1081],
        "validation_loss": [1.403133, 1.299911, 1.297571, 1.303477, 1.317898],
        "accuracy": [0.599824, 0.620932, 0.619173, 0.616315, 0.618294]
    },
    {
        "epoch": [1, 2, 3, 4, 5],
        "training_loss": [1.672, 1.423, 1.3074, 1.2275, 1.1963],
        "validation_loss": [1.450145, 1.345138, 1.313897, 1.310534, 1.322543],
        "accuracy": [0.591249, 0.613456, 0.613896, 0.617634, 0.612137]
    },
    {
        "epoch": [1, 2, 3, 4, 5],
        "training_loss": [2.1256, 1.5468, 1.4106, 1.3512, 1.2939],
        "validation_loss": [1.55336, 1.394838, 1.365641, 1.335207, 1.346485],
        "accuracy": [0.57212, 0.602243, 0.602023, 0.614116, 0.611478]
    }
]

# Create a 2x2 plot
fig, axs = plt.subplots(1, 2, figsize=(10, 4))

# Colors for different models
colors = ['b', 'g', 'r']
labels = ['16', '32', '64']

# Plot training/validation loss
for i, model_data in enumerate(data):
    axs[0].plot(model_data['epoch'], model_data['training_loss'], f"{colors[i]}-", label=f"Training Loss ({labels[i]})")
    axs[0].plot(model_data['epoch'], model_data['validation_loss'], f"{colors[i]}--", label=f"Validation Loss ({labels[i]})")

axs[0].set_title("Training vs Validation Loss")
axs[0].set_xlabel("Epoch")
axs[0].set_ylabel("Loss")
axs[0].legend()
axs[0].grid(True)

# Plot accuracy
for i, model_data in enumerate(data):
    axs[1].plot(model_data['epoch'], model_data['accuracy'], f"{colors[i]}-", label=f"Accuracy ({labels[i]})")

axs[1].set_title("Accuracy")
axs[1].set_xlabel("Epoch")
axs[1].set_ylabel("Accuracy")
axs[1].legend()
axs[1].grid(True)

plt.tight_layout()
plt.show()

"""Optimizing weight decay"""

finetune_full(2e-5, 16, decay=0.3)
finetune_full(2e-5, 16, decay=0.6)
finetune_full(2e-5, 16, decay=0.95)

# Data for each experiment
epochs = [1, 2, 3, 4, 5]

# Training and Validation Losses
train_loss_1 = [1.553800, 1.324700, 1.251200, 1.158500, 1.108100]
val_loss_1 = [1.403133, 1.299911, 1.297571, 1.303477, 1.317898]

train_loss_2 = [1.554600, 1.329300, 1.245300, 1.149100, 1.104800]
val_loss_2 = [1.415019, 1.301961, 1.287766, 1.286513, 1.300375]

train_loss_3 = [1.551800, 1.322500, 1.248200, 1.152900, 1.099800]
val_loss_3 = [1.401967, 1.297189, 1.294685, 1.301087, 1.313364]

train_loss_4 = [1.550700, 1.321500, 1.247100, 1.150300, 1.095700]
val_loss_4 = [1.401276, 1.295856, 1.292852, 1.299564, 1.310375]

# Accuracy data
accuracy_1 = [0.599824, 0.620932, 0.619173, 0.616315, 0.618294]
accuracy_2 = [0.595207, 0.615215, 0.622032, 0.626869, 0.624011]
accuracy_3 = [0.600044, 0.620712, 0.619393, 0.618514, 0.616755]
accuracy_4 = [0.601143, 0.620712, 0.620273, 0.617634, 0.616095]

# Plotting training/validation loss
plt.figure(figsize=(10, 4))

# Subplot for losses
plt.subplot(1, 2, 1)
plt.plot(epochs, train_loss_1, label='Train Loss (0.01)')
plt.plot(epochs, val_loss_1, label='Val Loss (0.01)', linestyle='--')
plt.plot(epochs, train_loss_2, label='Train Loss (0.3)')
plt.plot(epochs, val_loss_2, label='Val Loss (0.3)', linestyle='--')
plt.plot(epochs, train_loss_3, label='Train Loss (0.6)')
plt.plot(epochs, val_loss_3, label='Val Loss (0.6)', linestyle='--')
plt.plot(epochs, train_loss_4, label='Train Loss (0.95)')
plt.plot(epochs, val_loss_4, label='Val Loss (0.95)', linestyle='--')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend(fontsize=8)

# Subplot for accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs, accuracy_1, label='Accuracy (0.01)')
plt.plot(epochs, accuracy_2, label='Accuracy (0.3)')
plt.plot(epochs, accuracy_3, label='Accuracy (0.6)')
plt.plot(epochs, accuracy_4, label='Accuracy (0.95)')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.legend(fontsize=8)

# Display the plots
plt.tight_layout()
plt.show()

finetune_full(2e-5, 16, val=False ,decay=0.3, epochs=4)

finetune_full(2e-5, 16, val=False ,decay=0.3, average="macro", epochs=4, save=True)

"""Now I save the model to colab and use it to get the self-attention layers/heads later"""

model = AutoModelForSequenceClassification.from_pretrained('/content/fine_tuned_distilgpt2')
tokenizer = AutoTokenizer.from_pretrained('/content/fine_tuned_distilgpt2')

trainer = Trainer(model=model)
predictions = trainer.predict(test_dataset)

logits = predictions.predictions
true_labels = predictions.label_ids
true_labels2 = np.squeeze(true_labels, axis = 1)

predicted_labels = logits.argmax(axis=-1)
print(predicted_labels)
print(true_labels2)

correct_indices = (predicted_labels == true_labels2).nonzero()[0]  # Indices of correct predictions
incorrect_indices = (predicted_labels != true_labels2).nonzero()[0]
print(correct_indices)
print(incorrect_indices)

"""we see that sentences 0 and 2 are incorrectly classified and 1 and 3 are correctly classfified. we will use these"""

print("Incorrect:\n")

print(f'{test_set["text"][0]}, label: {test_set["labels"][0]}')
print(f'{test_set["text"][2]}, label: {test_set["labels"][2]}')

print("\nCorrect:\n")

print(f'{test_set["text"][1]}, label: {test_set["labels"][1]}')
print(f'{test_set["text"][3]}, label: {test_set["labels"][3]}')

# Tokenize the original text
special_token = "[CLS]"
tokenizer.add_special_tokens({"additional_special_tokens": [special_token]})
model.resize_token_embeddings(len(tokenizer))

# Prepend the `[CLS]` token to each sentence
correct_sentences = [f"{special_token} {test_set['text'][1]}", f"{special_token} {test_set['text'][3]}"]
incorrect_sentences = [f"{special_token} {test_set['text'][0]}", f"{special_token} {test_set['text'][2]}"]

# Tokenize with padding and truncation
correct_inputs = tokenizer(correct_sentences[0], padding=True, truncation=True, return_tensors="pt")
correct_inputs1 = tokenizer(correct_sentences[1], padding=True, truncation=True, return_tensors="pt")
incorrect_inputs = tokenizer(incorrect_sentences[0], padding=True, truncation=True, return_tensors="pt")
incorrect_inputs1 = tokenizer(incorrect_sentences[1], padding=True, truncation=True, return_tensors="pt")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
correct_outputs = model(**correct_inputs.to(device), output_attentions=True)
incorrect_outputs = model(**incorrect_inputs.to(device), output_attentions=True)
correct_outputs1 = model(**correct_inputs1.to(device), output_attentions=True)
incorrect_outputs1 = model(**incorrect_inputs1.to(device), output_attentions=True)

correct_attentions = correct_outputs.attentions
incorrect_attentions = incorrect_outputs.attentions
correct_attentions1 = correct_outputs1.attentions
incorrect_attentions1 = incorrect_outputs1.attentions

layer_idx = 0  # First transformer layer
head_idx = 0   # First attention head
correct_attention_matrix = correct_attentions[layer_idx][:, head_idx, :, :].detach().cpu().numpy()
incorrect_attention_matrix = incorrect_attentions[layer_idx][:, head_idx, :, :].detach().cpu().numpy()
correct_attention_matrix1 = correct_attentions1[layer_idx][:, head_idx, :, :].detach().cpu().numpy()
incorrect_attention_matrix1 = incorrect_attentions1[layer_idx][:, head_idx, :, :].detach().cpu().numpy()

tokens_correct = tokenizer.convert_ids_to_tokens(correct_inputs['input_ids'][0])
tokens_incorrect = tokenizer.convert_ids_to_tokens(incorrect_inputs['input_ids'][0])
tokens_correct1 = tokenizer.convert_ids_to_tokens(correct_inputs1['input_ids'][0])
tokens_incorrect1 = tokenizer.convert_ids_to_tokens(incorrect_inputs1['input_ids'][0])

# Number of layers and attention heads to iterate over
num_heads = correct_attentions[0].shape[1]  # Number of heads in layer 0

# Function to plot the attention matrix
def plot_attention_matrix(attention_matrix, tokens, title, ax):
    ax.imshow(attention_matrix, cmap='viridis', aspect='auto')
    ax.set_xticks(np.arange(len(tokens)))
    ax.set_xticklabels(tokens, rotation=90)
    ax.set_yticks(np.arange(len(tokens)))
    ax.set_yticklabels(tokens)
    ax.set_title(title)
    plt.colorbar(ax.imshow(attention_matrix, cmap='viridis', aspect='auto'), ax=ax)

# Define the layout of the subplot grid (3 rows, 4 columns)
rows, cols = 3, 4

# Create a subplot grid for the attention heads
fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(16, 12))

# Convert token IDs to tokens for the first sentence (adjust as needed)
tokens_correct = tokenizer.convert_ids_to_tokens(correct_inputs['input_ids'][0])

# Loop through heads in layer 0 and plot them in the appropriate subplot
for head_idx in range(num_heads):
    # Get attention matrix for the correct sentence for the given head in layer 0
    correct_attention_matrix = correct_attentions[0][:, head_idx, :, :].detach().cpu().numpy()

    # Calculate the row and column in the grid where the head should be plotted
    row_idx = head_idx // cols  # Row index for the subplot
    col_idx = head_idx % cols   # Column index for the subplot

    # Plot the attention matrix for the correct sentence in the corresponding subplot
    plot_attention_matrix(correct_attention_matrix[0], tokens_correct, f"Head {head_idx}", axes[row_idx, col_idx])

# Adjust layout and display the plot
plt.tight_layout()
plt.show()

rows, cols = 3, 4

fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(16, 12))

for head_idx in range(num_heads):
    correct_attention_matrix1 = correct_attentions1[0][:, head_idx, :, :].detach().cpu().numpy()

    row_idx = head_idx // cols
    col_idx = head_idx % cols

    plot_attention_matrix(correct_attention_matrix1[0], tokens_correct1, f"Head {head_idx}", axes[row_idx, col_idx])

plt.tight_layout()
plt.show()

rows, cols = 3, 4

fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(16, 12))

for head_idx in range(num_heads):
    incorrect_attention_matrix = incorrect_attentions[0][:, head_idx, :, :].detach().cpu().numpy()

    row_idx = head_idx // cols
    col_idx = head_idx % cols

    plot_attention_matrix(incorrect_attention_matrix[0], tokens_incorrect, f"Head {head_idx}", axes[row_idx, col_idx])

plt.tight_layout()
plt.show()

rows, cols = 3, 4

fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(20, 15))

num_heads = incorrect_attentions1[0].shape[1]

for head_idx in range(num_heads):
    incorrect_attention_matrix1 = incorrect_attentions1[0][:, head_idx, :, :].detach().cpu().numpy()

    row_idx = head_idx // cols
    col_idx = head_idx % cols

    plot_attention_matrix(incorrect_attention_matrix1[0], tokens_incorrect1, f"Head {head_idx}", axes[row_idx, col_idx])

plt.tight_layout()
plt.show()

# Load the tokenizer for BERT
tokenizer = AutoTokenizer.from_pretrained(bert_model)

# Add pad token to tokenizer's vocabulary if it's not already there
tokenizer.add_special_tokens({'pad_token': '[PAD]'})

# Set the pad_token to the newly added token
tokenizer.pad_token = '[PAD]'

train_set["tokenized"] = train_set["text"].apply(
    lambda x: tokenizer(x, padding="max_length", truncation=True, return_tensors="pt", max_length=128)
)

# Extract tokenized columns into new DataFrame columns
train_set["input_ids"] = train_set["tokenized"].apply(lambda x: x["input_ids"].squeeze(0))
train_set["attention_mask"] = train_set["tokenized"].apply(lambda x: x["attention_mask"].squeeze(0))

# Convert the padded sequences to PyTorch tensors
input_ids = torch.stack(list(train_set["input_ids"]))
attention_mask = torch.stack(list(train_set["attention_mask"]))
labels = torch.tensor(train_set["labels"].tolist())

test_set["tokenized"] = test_set["text"].apply(
    lambda x: tokenizer(x, padding="max_length", truncation=True, return_tensors="pt", max_length=128)
)

# Extract tokenized columns into new DataFrame columns
test_set["input_ids"] = test_set["tokenized"].apply(lambda x: x["input_ids"].squeeze(0))
test_set["attention_mask"] = test_set["tokenized"].apply(lambda x: x["attention_mask"].squeeze(0))

# Convert the padded sequences to PyTorch tensors
input_ids2 = torch.stack(list(test_set["input_ids"]))
attention_mask2 = torch.stack(list(test_set["attention_mask"]))
labels2 = torch.tensor(test_set["labels"].tolist())

val_set["tokenized"] = val_set["text"].apply(
    lambda x: tokenizer(x, padding="max_length", truncation=True, return_tensors="pt", max_length=128)
)

# Extract tokenized columns into new DataFrame columns
val_set["input_ids"] = val_set["tokenized"].apply(lambda x: x["input_ids"].squeeze(0))
val_set["attention_mask"] = val_set["tokenized"].apply(lambda x: x["attention_mask"].squeeze(0))

# Convert the padded sequences to PyTorch tensors
input_ids3 = torch.stack(list(val_set["input_ids"]))
attention_mask3 = torch.stack(list(val_set["attention_mask"]))
labels3 = torch.tensor(val_set["labels"].tolist())

train_dataset = Dataset.from_dict({
    "input_ids": input_ids.tolist(),
    "attention_mask": attention_mask.tolist(),
    "labels": labels.tolist(),
})

test_dataset = Dataset.from_dict({
    "input_ids": input_ids2.tolist(),
    "attention_mask": attention_mask2.tolist(),
    "labels": labels2.tolist(),
})

val_dataset = Dataset.from_dict({
    "input_ids": input_ids3.tolist(),
    "attention_mask": attention_mask3.tolist(),
    "labels": labels3.tolist(),
})

finetune_full(2e-5, 16, val=False ,decay=0.95, epochs=2, model1=bert_model)

finetune_full(2e-5, 16, val=False ,decay=0.95, average="macro", epochs=2, model1=bert_model)